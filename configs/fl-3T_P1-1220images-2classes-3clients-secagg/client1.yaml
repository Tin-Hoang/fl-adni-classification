data:
  train_csv_path: "data/ADNI/LABELS/3T_bl_org_1220images_3clients/3T_bl_org_MRI_UniqueSID_1220images_client_1_train_285images.csv"
  val_csv_path: "data/ADNI/LABELS/3T_bl_org_1220images_3clients/3T_bl_org_MRI_UniqueSID_1220images_client_1_val_61images.csv"
  img_dir: "data/ADNI/3T_bl_org_P1_20250603/3T_bl_org_MRI_1_NIfTI_removedDup_step3_skull_stripping"
  dataset_type: "normal"  # Options: "smartcache", "cache", "normal"
  classification_mode: "CN_AD"  # Options: "CN_MCI_AD" (3 classes), "CN_AD" (2 classes, where MCI is treated as CN)
  mci_subtype_filter: ["LMCI", "AD"]  # Optional: Filter MCI samples by subtypes. Can be single subtype or list. Valid: "SMC", "EMCI", "LMCI". Use null to include all MCI samples.
  resize_size: [182, 218, 182]  # Default resize dimensions (height, width, depth)
  resize_mode: "trilinear"  # Default resize mode
  use_spacing: false  # Disable spacing transform since images are already preprocessed
  spacing_size: [1.0, 1.0, 1.0]  # Original spacing in mm (not used when use_spacing is false)
  cache_rate: 0.0  # No cache for FL setting to save memory
  cache_num_workers: 1  # Workers for creating the cache (separate from DataLoader workers)
  transform_device: "cpu"  # Device to use for transforms (e.g., "cuda" or "cpu")
  multiprocessing_context: "fork"  # Options: "spawn", "fork", "forkserver"

model:
  name: "securefed_cnn"  # Using our secure federated CNN model
  num_classes: 2  # CN, MCI, AD - will be automatically adjusted based on classification_mode
  pretrained_checkpoint: null

training:
  batch_size: 2
  learning_rate: 0.0001
  weight_decay: 0.01  # 1e-2
  num_workers: 2
  seed: 42  # Random seed for reproducibility
  output_dir: "checkpoints/client1"
  mixed_precision: false
  visualize: false
  use_class_weights: true
  class_weight_type: "inverse"
  # manual_class_weights: [5.1741293532338308, 0.7283950617283951, 5.289617486338798]
  gradient_accumulation_steps: 1
  num_epochs: 1  # Dummy value to satisfy the config, the actual number of epochs is set in the server config
  lr_scheduler: "cosine"  # Options: "cosine", "step", "multistep", "plateau", "exponential", or empty/none for no scheduler

# Wandb logging configuration
wandb:
  use_wandb: true
  project: "fl-adni-classification"
  entity: "tin-hoang"
  tags: ["federated-learning", "adni", "securefed-cnn"]
  notes: "fl-securefed-secagg-3clients-1220images-2classes-LMCI-Client1"
  run_name: "fl-securefed-3T_P1-1220images-2classes-3clients-secagg-LMCI-Client1"

# FL-specific settings
fl:
  client_id: 1
  local_epochs: 1  # Number of local epochs per round
  evaluate_frequency: 5  # Evaluate every 10 rounds (can be overridden by server config)
  strategy: "secagg"           # FL aggregation strategy (options: "fedavg", "fedprox")

data:
  train_csv_path: "data/ADNI/LABELS/3T_bl_org_1220images_3clients/3T_bl_org_MRI_UniqueSID_1220images_client_3_train_284images.csv"
  val_csv_path: "data/ADNI/LABELS/3T_bl_org_1220images_3clients/3T_bl_org_MRI_UniqueSID_1220images_client_3_val_60images.csv"
  img_dir: "data/ADNI/3T_bl_org_P1_20250603/3T_bl_org_MRI_1_NIfTI_removedDup_step3_skull_stripping"
  dataset_type: "normal"  # Options: "smartcache", "cache", "normal"
  classification_mode: "CN_AD"  # Options: "CN_MCI_AD" (3 classes), "CN_AD" (2 classes, where MCI is treated as CN)
  mci_subtype_filter: ["LMCI", "AD"]  # Optional: Filter MCI samples by subtypes. Can be single subtype or list. Valid: "SMC", "EMCI", "LMCI". Use null to include all MCI samples.
  resize_size: [96, 96, 73]  # Default resize dimensions (height, width, depth)
  resize_mode: "trilinear"  # Default resize mode
  use_spacing: false  # Disable spacing transform since images are already preprocessed
  spacing_size: [1.0, 1.0, 1.0]  # Original spacing in mm (not used when use_spacing is false)
  cache_rate: 0.0  # No cache for FL setting to save memory
  cache_num_workers: 1  # Workers for creating the cache (separate from DataLoader workers)
  transform_device: "cuda"  # Device to use for transforms (e.g., "cuda" or "cpu")
  multiprocessing_context: "spawn"  # Options: "spawn", "fork", "forkserver"

model:
  name: "rosanna_cnn"  # Using our secure federated CNN model
  num_classes: 2  # CN, MCI, AD - will be automatically adjusted based on classification_mode
  pretrained_checkpoint: null
  # Pretrained model specific parameters
  freeze_encoder: false  # Set to true for feature extraction only
  dropout: 0.0  # Dropout rate for regularization
  input_channels: 1  # Number of input channels

training:
  batch_size: 16
  learning_rate: 0.0001
  weight_decay: 0.01  # 1e-2
  num_workers: 8
  seed: 42  # Random seed for reproducibility
  output_dir: "outputs"
  mixed_precision: false
  visualize: false
  use_class_weights: true
  class_weight_type: "inverse"
  # manual_class_weights: [5.1890547263681592, 0.6694677871148459, 5.5031446540880504]
  gradient_accumulation_steps: 1
  num_epochs: 1  # Dummy value to satisfy the config, the actual number of epochs is set in the server config
  lr_scheduler: "cosine"  # Options: "cosine", "step", "multistep", "plateau", "exponential", or empty/none for no scheduler
  loss_type: "cross_entropy"  # Options: "cross_entropy", "focal"
  focal_alpha: null   # Alpha parameter for Focal Loss (typically 0.25-1.0)
  focal_gamma: null    # Gamma parameter for Focal Loss (typically 0.5-5.0)

# Wandb logging configuration
wandb:
  use_wandb: true
  project: "fl-adni-classification"
  entity: "tin-hoang"
  tags: ["federated-learning", "fedprox", "rosanna"]
  notes: "fl-rosanna-3T_P1-1220images-2classes-3clients-fedprox"
  run_name: "fl-rosanna-3T_P1-1220images-2classes-3clients-fedprox"

# FL-specific settings
fl:
  client_id: 3
  local_epochs: 1  # Number of local epochs per round
  num_rounds: 200  # Number of FL rounds to perform (to setup LR scheduler)
  evaluate_frequency: 5  # Evaluate every 10 rounds (can be overridden by server config)
  strategy: "fedprox"           # FL aggregation strategy (options: "fedavg", "fedprox")
  fedprox_mu: 0.1  # FedProx regularization parameter
